---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am Yangle Liu, an undergraduate student at the University of Liverpool. My research spans computer vision/graphics and machine learning, with interests in real-time scene understanding and editing, high-fidelity rendering, and data-driven methods for robust visual computing.

I am supervised by [Prof. Jieming Ma](https://scholar.xjtlu.edu.cn/en/persons/JiemingMa) at Xiâ€™an Jiaotongâ€“Liverpool University (XJTLU) and [Prof. Dominik Wojtczak](https://www.csc.liv.ac.uk/~dominik/) at the University of Liverpool. In addition, I have actively collaborated with several international researchers with Prof. [Prof. Hai-Ning Liang](https://cma.hkust-gz.edu.cn/people/hai-ning-liang/) (HKUST (Guangzhou)), [Prof. Yaochun Shen](https://www.liverpool.ac.uk/people/yaochun-shen) (University of Liverpool), and [Postdoc. Bo Xiong](https://boxiong.io/) (Stanford University), with whom I maintain close academic collaboration.


# ğŸ”¥ News
- *2025.08*: &nbsp;ğŸ‰ğŸ‰ My collaborative paper will be presented as an Oral Presentation at BDAI, 2025! 
- *2024.11*: &nbsp;ğŸ‰ğŸ‰ The first collaborative paper I worked on was accepted by ICSTIS, 2024!

# ğŸ“ Publications 
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv preprint</div><img src='/images/fruit.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[CountingFruit: Real-Time 3D Fruit Counting with Language-Guided Semantic Gaussian Splatting](https://arxiv.org/abs/2506.01109)


Fengze Li, **Yangle Liu**, Jieming Ma*, Hai-Ning Liang, Yaochun Shen, Huangxiang Li, Zhijing Wu 

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">BDAI 2025</div><img src='/images/seed.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[SEED: A Structural Encoder for Embedding-Driven Decoding in Time Series Prediction with LLMs](https://arxiv.org/abs/2506.20167)

<span style="color:red">(Oral Presentation)</span>

Fengze Li, Yue Wang, **Yangle Liu**, Dou Hong, Jieming Ma*, Huangxiang Li 

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div>




<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM TMCCA 2024</div><img src='/images/EAST.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

EAST: Environment-Aware Stylized Transition Along the Reality-Virtuality Continuum

*Under Review*

Xiaohan Zhang, Kan Liu, **Yangle Liu**, Fangze Li, Jieming Ma and Yue Li* 

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div>





<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICSTIS 2024</div><img src='/images/car.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Automated Generation of Parking Data Sets for Underground Car Parks](https://doi.org/10.4271/2025-01-7191)


Jiakai Li, **Yangle Liu**, Zheng Rong

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div>



# ğŸ– Honors and Awards

- *2023* RoboMaster 2023 Master Super Competition: First Prize (Regional), Second Prize (National) 
- *2023* College Student Information System Innovation Competition: Outstanding Award
- *2023* Campus Technology Innovation Association: Managed 3D Modeling Project
- *2022* Campus Ambassador for XJTLU: Achieved promotional targets with six schools, received Pioneer Award 

# ğŸ“– Educations
- *2024.09 - now*, Undergraduate, University of Liverpool, UK.
- *2022.09 - 2024.06*, Undergraduate, Xiâ€™an Jiaotongâ€“Liverpool University, Suzhou.



# ğŸ’» Internships
<div class='paper-box'><div class='paper-box-image'><div><img src='/images/Neurova.jpg' alt="sym" style="width:150px; height:100px; object-fit:cover;"></div></div>
<div class='paper-box-text' markdown="1">

Neurova, New York

Machine Learning Intern 

*2025.06 - 2025.08*

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><img src='/images/auo (2).png' alt="sym" style="width:100px; height:100px; object-fit:cover;"></div></div>
<div class='paper-box-text' markdown="1">

AUO, Suzhou

Software Development Intern, NUVA Platform Development Department 

*2024.06 - 2024.08*

</div>
</div>


<!-- å°å°ºå¯¸å®¹å™¨ï¼Œæ”¾åˆ°å³ä¸‹è§’ï¼›ä½ ä¹Ÿå¯ä»¥æ”¹æˆå±…ä¸­æˆ–å›ºå®šåœ¨åˆ«å¤„ -->
<div id="globeViz"
     style="position:fixed; right:16px; bottom:16px; width:320px; height:320px; z-index:10;"></div>

<script src="https://unpkg.com/globe.gl"></script>
<script>
  const globe = Globe()
    // â‘  èƒŒæ™¯æ¢ä¸ºç™½è‰²
    .backgroundColor('#ffffff')                          // API æ”¯æŒè®¾ç½®èƒŒæ™¯è‰²
    // â‘¡ ä½¿ç”¨æ›´äº®çš„â€œç™½å¤©çº¹ç†â€ï¼ˆæ›¿æ¢ä½ ä¹‹å‰çš„ earth-dark.jpgï¼‰
    .globeImageUrl('//unpkg.com/three-globe/example/img/earth-day.jpg')
    // â‘¢ è°ƒå°ç”»å¸ƒå°ºå¯¸ï¼ˆä¹Ÿå¯åªæ”¹ä¸Šé¢çš„ div å®½é«˜ï¼‰
    .width(320)
    .height(320)
    // â‘£ï¼ˆå¯é€‰ï¼‰æŠŠå¤§æ°”å±‚é¢œè‰²è°ƒæµ…ä¸€ç‚¹ï¼Œæ˜¾å¾—æ›´äº®
    .atmosphereColor('lightskyblue')
    .atmosphereAltitude(0.2);

  globe(document.getElementById('globeViz'))
    // ä»ç„¶ä½¿ç”¨ä½ ä¹‹å‰çš„åŸå¸‚åæ ‡
    .pointsData([
      { lat: 31.3017, lng: 120.5811, name: 'Suzhou' },
      { lat: 31.00,   lng: 121.25,   name: 'Shanghai' },
      { lat: 23.128994, lng: 113.253250, name: 'Guangzhou' },
      { lat: 37.7749,   lng: -122.4194,  name: 'San Francisco' },
      { lat: 53.4084,   lng: -2.9916,    name: 'Liverpool' },
      { lat: 51.5074,   lng: -0.1278,    name: 'London' },
      { lat: 52.2053,   lng:  0.1218,    name: 'Cambridge' },
      { lat: 48.7758,   lng:  9.1829,    name: 'Stuttgart' }
    ])
    .pointColor(() => 'green')
    .pointRadius(0.5)
    .pointLabel(d => d.name);

  // â‘¤ è‡ªåŠ¨æ—‹è½¬ï¼ˆå¯è°ƒé€Ÿåº¦ï¼‰
  globe.controls().autoRotate = true;
  globe.controls().autoRotateSpeed = 0.6;  // 0.2~1.0 ä¹‹é—´æ¯”è¾ƒèˆ’é€‚
</script>

